{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d7dc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d988a",
   "metadata": {},
   "source": [
    "### 1. 自动求梯度简介\n",
    "- 在深度学习中，我们经常需要对函数求梯度（gradient）。本节将介绍如何使用tensorflow2.0提供的GradientTape来自动求梯度。\n",
    "    GradientTape 可以理解为“梯度流 记录磁带”：\n",
    "\n",
    "- 在记录阶段：记录被 GradientTape 包裹的运算过程中，依赖于 source node （被 watch “监视”的变量）的关系图。\n",
    "\n",
    "- 在求导阶段：通过搜索 source node 到 target node 的路径，进而计算出偏微分。\n",
    "\n",
    "- source node 在记录运算过程之前进行指定：\n",
    "\n",
    "- 自动“监控”所有可训练变量：GradientTape 默认（watch_accessed_variables=True）将所有可训练变量（created by tf.Variable, where trainable=True）视为需要“监控”的 source node 。\n",
    "\n",
    "- 对于不可训练的变量（比如tf.constant）可以使用tape.watch()对其进行“监控”。\n",
    "\n",
    "- 此外，还可以设定watch_accessed_variables=False，然后使用tf.watch()精确控制需要“监控”哪些变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4eab4",
   "metadata": {},
   "source": [
    "### 2. 简单示例\n",
    "- 对函数 $y = 2x^Tx$ 求关于向量 $x$ 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d6b482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建变量并赋初值\n",
    "\n",
    "x = tf.reshape(tf.constant(range(4),dtype='float32'), (4,1))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa945c2",
   "metadata": {},
   "source": [
    "- 函数 $y = 2x^Tx$ 的梯度应该是 4$x$\n",
    "-  t.watch(x) : 对于Variable类型的变量，一般不用加此监控 (source node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b369ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[ 0.],\n",
       "       [ 4.],\n",
       "       [ 8.],\n",
       "       [12.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(x) #对于Variable类型的变量，一般不用加此监控 (source node)\n",
    "    y = 2 * tf.matmul(tf.transpose(x),x)\n",
    "\n",
    "g_dx = t.gradient(y,x)\n",
    "g_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b615e69",
   "metadata": {},
   "source": [
    "### 3.  训练模式和预测模式\n",
    "- 默认情况下， GradientTape 持有的资源会在 GradientTape.gradient() 方法被调用后立即释放。 要在同一计算中计算多个梯度，设置`tf.GradientTape(persistent=True)`。 他可以让 Tape 对象被垃圾回收时释放资源时多次调用 gradient() 方法。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda8f031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[  0.],\n",
       "        [  4.],\n",
       "        [ 32.],\n",
       "        [108.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[0.],\n",
       "        [2.],\n",
       "        [4.],\n",
       "        [6.]], dtype=float32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y \n",
    "    dz_dx = g.gradient(z,x)  # 108.0 (4*x^3 at x = 3) 注意这个不是矩阵乘积\n",
    "    dy_dx = g.gradient(y,x)\n",
    "\n",
    "dz_dx,dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdd185",
   "metadata": {},
   "source": [
    "### 4. 对Python函数控制流求梯度\n",
    "- 即使函数的计算图包含了Python的控制流（如条件和循环控制），我们也有可能对变量求梯度。\n",
    "- 需要强调的是，这里循环（while循环）迭代的次数和条件判断（if语句）的执行都取决于输入a的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd4713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a):\n",
    "    b = a * 2\n",
    "    # 计算b向量的范数\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    # 对b中所有元素求和\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d53713f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.5131821]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal((1,1),dtype=tf.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3922ed",
   "metadata": {},
   "source": [
    "- 我们来分析一下上面定义的f函数。事实上，给定任意输入a，其输出必然是 f(a) = x * a的形式，其中标量系数x的值取决于输入a。由于c = f(a)有关a的梯度为x，且值为c / a，我们可以像下面这样验证对本例中控制流求梯度的结果的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2422e4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(a)\n",
    "    c = func(a)\n",
    "\n",
    "t.gradient(c,a) == (c / a) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430e15a",
   "metadata": {},
   "source": [
    "### 5. 文档查阅\n",
    "- 当我们想知道一个模块里面提供了哪些可以调用的函数和类的时候，可以使用dir函数。下面我们打印dtypes和random模块中所有的成员或属性。\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb86d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DType',\n",
       " 'QUANTIZED_DTYPES',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'as_dtype',\n",
       " 'bfloat16',\n",
       " 'bool',\n",
       " 'cast',\n",
       " 'complex',\n",
       " 'complex128',\n",
       " 'complex64',\n",
       " 'double',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'half',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'qint16',\n",
       " 'qint32',\n",
       " 'qint8',\n",
       " 'quint16',\n",
       " 'quint8',\n",
       " 'resource',\n",
       " 'saturate_cast',\n",
       " 'string',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'variant']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3fa78",
   "metadata": {},
   "source": [
    "- 通常我们可以忽略掉由__开头和结尾的函数（Python的特别对象）或者由_开头的函数（一般为内部函数）。通过其余成员的名字我们大致猜测出这个模块提供了各种随机数的生成方法，包括从均匀分布采样（uniform）、从正态分布采样（normal）、从泊松分布采样（poisson）等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1d1fa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Algorithm',\n",
       " 'Generator',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'all_candidate_sampler',\n",
       " 'categorical',\n",
       " 'create_rng_state',\n",
       " 'experimental',\n",
       " 'fixed_unigram_candidate_sampler',\n",
       " 'gamma',\n",
       " 'get_global_generator',\n",
       " 'learned_unigram_candidate_sampler',\n",
       " 'log_uniform_candidate_sampler',\n",
       " 'normal',\n",
       " 'poisson',\n",
       " 'set_global_generator',\n",
       " 'set_seed',\n",
       " 'shuffle',\n",
       " 'stateless_binomial',\n",
       " 'stateless_categorical',\n",
       " 'stateless_gamma',\n",
       " 'stateless_normal',\n",
       " 'stateless_parameterized_truncated_normal',\n",
       " 'stateless_poisson',\n",
       " 'stateless_truncated_normal',\n",
       " 'stateless_uniform',\n",
       " 'truncated_normal',\n",
       " 'uniform',\n",
       " 'uniform_candidate_sampler']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6e111",
   "metadata": {},
   "source": [
    "- 想了解某个函数或者类的具体用法时，可以使用help函数。让我们以ones函数为例，查阅它的用法。更详细的信息，可以通过Tensorflow的 [API文档](https://www.tensorflow.org/versions) 版本选择页，选择与自己环境中的 tensorflow 版本一致的 API 版本进行查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a2cd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ones in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "ones(shape, dtype=tf.float32, name=None)\n",
      "    Creates a tensor with all elements set to one (1).\n",
      "    \n",
      "    See also `tf.ones_like`, `tf.zeros`, `tf.fill`, `tf.eye`.\n",
      "    \n",
      "    This operation returns a tensor of type `dtype` with shape `shape` and\n",
      "    all elements set to one.\n",
      "    \n",
      "    >>> tf.ones([3, 4], tf.int32)\n",
      "    <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
      "    array([[1, 1, 1, 1],\n",
      "           [1, 1, 1, 1],\n",
      "           [1, 1, 1, 1]], dtype=int32)>\n",
      "    \n",
      "    Args:\n",
      "      shape: A `list` of integers, a `tuple` of integers, or\n",
      "        a 1-D `Tensor` of type `int32`.\n",
      "      dtype: Optional DType of an element in the resulting `Tensor`. Default is\n",
      "        `tf.float32`.\n",
      "      name: Optional string. A name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with all elements set to one (1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd6d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
